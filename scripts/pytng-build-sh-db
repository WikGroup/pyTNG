#!/usr/bin/env python

#----------------------------------------------------------------------------------------------------------------------#
# pytng-build-sh-db command line script.                                                                               #
#----------------------------------------------------------------------------------------------------------------------#

from argparse import ArgumentParser
import threading
from tqdm.auto import tqdm
from pyTNG.utility.utils import mylog,user_api_key
from pyTNG.utility.process_functions import write_subhalos_to_sql
from pyTNG._backend.api import TNGSnapshotAPI
import os
import numpy as np
import requests
from concurrent.futures import ThreadPoolExecutor
import pathlib as pt
import sqlalchemy as sql
from itertools import repeat

# Setting up command line options
#================================#
parser = ArgumentParser()

# positional arguments
parser.add_argument("simulation",
                    type=str,
                    help="The simulation to search."
                    )
parser.add_argument("snapshot_id",
                    type=int,
                    help="The snapshot id to pull from. If not filled, the last available snapshot is used."
                    )

# keyword arguments
parser.add_argument("-o","--output",default=None,required=False,help="The output filename and directory.")
parser.add_argument("-m","--mass",default=None,required=False,help="Mass threshold (in log_10) at which to filter.")
parser.add_argument("-q","--quiet",default=False,required=False,action="store_true",help="Quiet the command output.")
parser.add_argument("-t","--threading",default=False,required=False,action="store_true",help="Enable threading")
parser.add_argument("-n","--nthreads",default=None,required=False,type=int,help="The number of allowed threads.")
parser.add_argument("-g","--group_size",default=1000,required=False,type=int,help="The default chunk size for querying the API.")
_args = parser.parse_args()

simulation_name,snapshot_id = _args.simulation,_args.snapshot_id
output,mass_threshold,quiet,thread,nthreads,group_size = _args.output,_args.mass,_args.quiet,_args.threading,_args.nthreads,_args.group_size

if output is None:
    output = os.path.join(os.getcwd(),f"{simulation_name}-SHDB.db")

if not thread:
    nthreads = 1
else:
    if nthreads is None:
        nthreads = 8 #hard set default.



# Managing utilities
#===================#
_cheader = "[\x1b[36;49mpytng\x1b[39;49m] - [\x1b[36;49mbuild-sh-db\x1b[39;49m]: "

# header print
#-------------#
print(f"{_cheader} Constructing sub-halo database of \x1b[32;49m{simulation_name}\x1b[39;49m...")
print(f"{_cheader} output file will be \x1b[32;49m{output}\x1b[39;49m.")

if mass_threshold is not None:
    print(f"{_cheader} Enforcing a log-mass threshold >= \x1b[32;49m{mass_threshold}\x1b[39;49m.")

if thread:
    print(f"{_cheader} THREADING=TRUE; Nthreads = \x1b[32;49m{nthreads}\x1b[39;49m.")
else:
    print(f"{_cheader} THREADING=FALSE.")


# Pulling the correct simulation / snapshot
#==========================================#
try:
    snap = TNGSnapshotAPI(simulation=simulation_name,snapshot=snapshot_id)
except requests.exceptions.HTTPError:
    print(f"{_cheader} [\x1b[31;49mERROR\x1b[39;49m] Simulation {simulation_name} couldn't be loaded. EXIT.")
    exit(1)

_total_number_subhalos = snap.attributes["num_groups_subfind"]
_base_api_url = snap.api_url +"/subhalos/"

genurl = lambda limit,offset: _base_api_url+f"?limit={limit}&offset={offset}"

print(f"{_cheader} Simulation {simulation_name} at snapshot {snapshot_id} has {_total_number_subhalos} subhalos.")

#-- splitting subhalos into their ID groups --#
offsets = np.arange(stop=_total_number_subhalos,step=group_size)
limit = [group_size]*len(offsets)
urls = [genurl(l,o) for o,l in zip(offsets,limit)]
print(f"{_cheader} NGROUPS: \x1b[32;49m{len(offsets)}\x1b[39;49m.")

# Generating the database file
#=============================#
if not os.path.exists(output):
    pt.Path(output).parents[0].mkdir(parents=True,exist_ok=True)

print(f"{_cheader} Creating database table.")

engine = sql.create_engine(f"sqlite:///{output}",echo=False)
meta_data = sql.MetaData()
columns = [
    sql.Column("id",sql.types.BIGINT),
    sql.Column("sfr",sql.types.FLOAT),
    sql.Column("mass_log_msun", sql.types.FLOAT),
    sql.Column("url", sql.types.TEXT)
]

_ = sql.Table(f"SHID_{simulation_name}_{snapshot_id}",meta_data,*columns)
meta_data.create_all(engine)

print(f"\r{_cheader} Creating database table. [\x1b[36;49mDONE\x1b[39;49m]")


progress_bar = tqdm(leave=True,total=_total_number_subhalos,desc=f"{_cheader} Querying subhalos...")

with ThreadPoolExecutor(max_workers=nthreads) as executor:
    # passing processes to the multithreading.
    results = executor.map(write_subhalos_to_sql,
                           repeat(engine),
                           repeat(f"SHID_{simulation_name}_{snapshot_id}"),
                           urls,
                           repeat(mass_threshold),
                           repeat(user_api_key),
                           repeat(progress_bar))
progress_bar.close()

count = 0
for res in results:
    count += res

print(f"{_cheader} FINISHED. Total tabulated subhalos: {count}.")
